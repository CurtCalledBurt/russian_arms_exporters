{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5 - Implementation on Ukrainian Dataset\n",
    "We used a Ukrainian trade dataset (`ukraine_trade_data_2018_ontology.csv`) as an input to our function to fully test our product.  \n",
    "\n",
    "This dataset was similar in structure to our training dataset in that it had columns for `COMPANY_NAME`, `COMPANY_ID`, and `DESCRIPTION`.  However there were some elements of the dataset that needed to be fixed before processing. The column names were also different than those of our training dataset, but our product allows for column specification, so this was not an issue.\n",
    "\n",
    "The results were positive!  Although a stronger metric is needed to identify similarity to known arms exporters, our product identified 32 INNs in the 7,326,528 row dataset as having similar text trading patterns as known Russian arms exporters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip installations - necessary to get notebook to run\n",
    "#update dask\n",
    "!pip install --upgrade pip\n",
    "!pip install dask==2.4.8\n",
    "!pip install fsspec\n",
    "!pip install --upgrade s3fs\n",
    "!pip install numpy\n",
    "!pip install pymystem3\n",
    "!pip install spacy\n",
    "!pip install joblib\n",
    "!pip install pymorphy2==0.8\n",
    "!pip install dask_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# dataframe\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# DESCRIPTION_GOOD preprocessing\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "# machine learning/analysis\n",
    "from dask_ml.cluster import KMeans as dask_ml_model # sklearn's skmeans took up too much memory to run.\n",
    "\n",
    "# measuring euclidian distance\n",
    "from scipy.spatial.distance import euclidean, pdist\n",
    "\n",
    "# S3 bucket interaction\n",
    "import tempfile\n",
    "import boto3\n",
    "import joblib\n",
    "\n",
    "# Disable warning message related to SettingWithCopyWarning\n",
    "# displays when running final function otherwise\n",
    "pd.options.mode.chained_assignment = None     # default = 'warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stemmer and Russian stopwords for data preprocessing\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "# https://stackoverflow.com/questions/5511708/adding-words-to-nltk-stoplist\n",
    "# add trade-specific stopwords to list\n",
    "newStopWords = ['г', '№', '10', '1', '20', '30', 'кг', '5', 'см',\n",
    "                '100', '80', '2', 'х', 'l', 'м', '00', '000'\n",
    "                '1.27', '2011.10631', '4', '12', '3', 'фр', 'количество',\n",
    "                'становиться', 'мм', 'вид', 'упаковка', 'получать',\n",
    "                'прочий', 'использование', 'масса', 'размер', 'черный',\n",
    "                '6', '8', '7', '50', '40', '25', 'коробка', 'поддон',\n",
    "                'вдоль', '250', '65', '85', '15', '35', '40', '45',\n",
    "                '55', '60', '70', '75', 'м3', '13', '0', '14',\n",
    "                '16', '18', 'm2', 'п', 'р', 'т', 'тип', 'являться',\n",
    "                'размер', 'cm', 'm', '01', '02', '03', '04', '05',\n",
    "                '06', '07', '08', '09', '24', '27']\n",
    "russian_stopwords.extend(newStopWords)\n",
    "\n",
    "#define function for preprocessing text - to be used later in notebook\n",
    "#function will remove Russian stop words and any punctuation not removed in cleaning_trade_data_desc_kmeans.ipynb\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "        and token != \" \" \\\n",
    "        and token.strip() not in punctuation]\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "# similarity function for euclidian measure at end of main function\n",
    "def similarity_func(u, v):\n",
    "    return 1/(1+euclidean(u,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('',dtype={'CONTRACTOR_ADDRESS': 'object', \n",
    "                                                                                       'EXCISE_DUTY_UAH': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TRADE_DIRECTION', 'SHIPPER_NAME', 'SHIPPER_EDRPOU',\n",
       "       'DESCRIPTION_GOODS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['TRADE_DIRECTION', 'SHIPPER_NAME', 'SHIPPER_EDRPOU', 'DESCRIPTION_GOODS']]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7326528, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[X['TRADE_DIRECTION'] == 'EXPORT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['TRADE_DIRECTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SHIPPER_NAME         object\n",
       "SHIPPER_EDRPOU       object\n",
       "DESCRIPTION_GOODS    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all EDRPOU in ukrainian dataset were floats before we converted them to strings, have '.0' at end\n",
    "# couldn't convert to int because of NaN values, didnt want to lose any data in dataset\n",
    "# .str[:-2] removes last two characters from every string in column, in this case '.0'\n",
    "X['SHIPPER_EDRPOU'] = X['SHIPPER_EDRPOU'].str[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHIPPER_NAME</th>\n",
       "      <th>SHIPPER_EDRPOU</th>\n",
       "      <th>DESCRIPTION_GOODS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ТОВАРИСТВО З ОБМЕЖЕНОЮ ВIДПОВIДАЛЬНIСТЮ \"ЕКО С...</td>\n",
       "      <td>40142870</td>\n",
       "      <td>1.МАКУХА СОЄВА НА КОРМА ДЛЯ ТВАРИН - 22000КГ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ПРИВАТНЕ АКЦIОНЕРНЕ ТОВАРИСТВО \"ГРАФІЯ УКРАЇНА\"</td>\n",
       "      <td>2469333</td>\n",
       "      <td>1.ВКЛАДИШІ ДО СИГАРЕТНОЇ ПАЧКИ: IC WI / IC WI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ПРИВАТНЕ АКЦIОНЕРНЕ ТОВАРИСТВО \"НОВИЙ СТИЛЬ\"</td>\n",
       "      <td>32565288</td>\n",
       "      <td>1.ГВИНТ З ЦИЛІНДРИЧНОЮ ГОЛ.ISO 4762-M8X2 0-8.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ТОВАРИСТВО З ОБМЕЖЕНОЮ ВIДПОВIДАЛЬНIСТЮ \"ТАРТУ...</td>\n",
       "      <td>38282429</td>\n",
       "      <td>1. ПЛОДИ СИРІ, ЗАМОРОЖЕНІ, БЕЗ ДОДАВАННЯ ЦУКРУ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ПРИВАТНЕ АКЦIОНЕРНЕ ТОВАРИСТВО \"МАРІУПОЛЬСЬКИЙ...</td>\n",
       "      <td>191129</td>\n",
       "      <td>1.ТРУБА ЕЛЕКТРОЗВАРНА КВАДРАТНОГО ПЕРЕРІЗУ ЗГІ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        SHIPPER_NAME SHIPPER_EDRPOU  \\\n",
       "0  ТОВАРИСТВО З ОБМЕЖЕНОЮ ВIДПОВIДАЛЬНIСТЮ \"ЕКО С...       40142870   \n",
       "1    ПРИВАТНЕ АКЦIОНЕРНЕ ТОВАРИСТВО \"ГРАФІЯ УКРАЇНА\"        2469333   \n",
       "2       ПРИВАТНЕ АКЦIОНЕРНЕ ТОВАРИСТВО \"НОВИЙ СТИЛЬ\"       32565288   \n",
       "3  ТОВАРИСТВО З ОБМЕЖЕНОЮ ВIДПОВIДАЛЬНIСТЮ \"ТАРТУ...       38282429   \n",
       "4  ПРИВАТНЕ АКЦIОНЕРНЕ ТОВАРИСТВО \"МАРІУПОЛЬСЬКИЙ...         191129   \n",
       "\n",
       "                                   DESCRIPTION_GOODS  \n",
       "0       1.МАКУХА СОЄВА НА КОРМА ДЛЯ ТВАРИН - 22000КГ  \n",
       "1  1.ВКЛАДИШІ ДО СИГАРЕТНОЇ ПАЧКИ: IC WI / IC WI ...  \n",
       "2  1.ГВИНТ З ЦИЛІНДРИЧНОЮ ГОЛ.ISO 4762-M8X2 0-8.8...  \n",
       "3  1. ПЛОДИ СИРІ, ЗАМОРОЖЕНІ, БЕЗ ДОДАВАННЯ ЦУКРУ...  \n",
       "4  1.ТРУБА ЕЛЕКТРОЗВАРНА КВАДРАТНОГО ПЕРЕРІЗУ ЗГІ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SHIPPER_NAME         1831495\n",
       "SHIPPER_EDRPOU       1831495\n",
       "DESCRIPTION_GOODS    1831495\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export ukraine to S3 bucket\n",
    "# df.to_csv('s3://labs20-arms-bucket/data/ukraine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model, vectorizer, and tokenizer to notebook\n",
    "s3 = boto3.resource('s3')\n",
    "bucket=s3.Bucket('labs20-arms-bucket')\n",
    "\n",
    "# load vectorizer from S3 bucket\n",
    "key = \"vectorizerf.pkl\"\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    bucket.download_fileobj(Fileobj=fp, Key=key)\n",
    "    fp.seek(0)\n",
    "    vectorizer = joblib.load(fp)\n",
    "\n",
    "# load model from S3 bucket\n",
    "key = \"modelf.pkl\"\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    bucket.download_fileobj(Fileobj=fp, Key=key)\n",
    "    fp.seek(0)\n",
    "    model = joblib.load(fp)\n",
    "\n",
    "#load cluster dataset from S3 bucket\n",
    "# drop error column accidentally created in import\n",
    "clusters = pd.read_csv('s3://labs20-arms-bucket/data/armsclustersf.csv')\n",
    "clusters = clusters.drop([clusters.columns[0]], axis='columns')\n",
    "\n",
    "# list of known arms exporters\n",
    "inn_arms_exp_total = ['7718852163',  '7740000090',    '7731084175',  '6161021690',\n",
    "                      '3807002509',  '6672315362',    '7802375335',  '7813132895',  \n",
    "                      '7731280660',  '7303026762',    '5040007594',  '2501002394',  \n",
    "                      '7807343496',  '7731559044',    '5042126251',  '7731595540',    \n",
    "                      '7733018650',  '7722016820',    '7705654132',  '7714336520',    \n",
    "                      '7801074335',  '6229031754',    '7830002462',  '6825000757',  \n",
    "                      '5043000212',  '7802375889',    '5010031470',  '1660249187',  \n",
    "                      '7720015691',  '6154573235',    '5038087144',  '7713006304',  \n",
    "                      '7805326230',  '5023002050',    '4007017378',  '7714013456',  \n",
    "                      '17718852163', '7811406004',    '7702077840',  '7839395419',  \n",
    "                      '7702244226',  '7704721192',    '7731644035',  '7712040285',\n",
    "                      '7811144648',  '4345047310',    '7720066255',  '6607000556',\n",
    "                      '1832090230',  '1835011597',    '3305004083',  '4340000830',\n",
    "                      '5074051432',  '1841015504',    '7105008338',  '7106002829', \n",
    "                      '7704274402',  '5942400228',    '7105514574',  '5012039795', \n",
    "                      '7714733528',  '3904065550',    '6825000757',  '7807343496', \n",
    "                      '7731559044',  '7805231691',    '7704859803',  '0273008320',\n",
    "                      '7704274402',  '2902059091',    '7805034277',  '7727692011',\n",
    "                      '7733759899',  '6154028021',    '7328032711',  '2635002815',\n",
    "                      '5040097816',  '5027033274',    '5250018433',  '5200000046',\n",
    "                      '7743813961',  '7718016666',    '5047118550',  '7704274402']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictor_function(df, name_column = 'CONSIGNOR_NAME', id_column = 'CONSIGNOR_INN', text_column = 'DESCRIPTION_GOOD',\n",
    "                              invalid_id_terms = ['None', '00', 'ИНН/КПП НЕ О', '0'], min_trades=40, profile_similarity_threshold = .75,\n",
    "                              cluster_columns = ['clust0', 'clust1', 'clust6']):\n",
    "    \"\"\"\n",
    "    function to clean INNs of input dataframe and return Russian arms exporter similarity calculation\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # set column variable\n",
    "        # reduce dataframe so that dataframe only contains columns in columns variable\n",
    "        df = df[[name_column, id_column, text_column]]\n",
    "        \n",
    "        # remove rows from dataset containing INNs of known arms exporters\n",
    "        # check 'INN' column against inn_arms_exp_total list, drop row if there's a match with the list\n",
    "        df = df[~df[id_column].isin(inn_arms_exp_total)]\n",
    "        \n",
    "        # clean INNs\n",
    "        # Create subslice of dataframe for dictionary\n",
    "        dict_df = df[[name_column, id_column]]\n",
    "        # clean columns of dict_df, remove invalid_id_terms from CONSIGNOR_INN column\n",
    "        invalid_id_terms = invalid_id_terms\n",
    "        for term in invalid_id_terms:\n",
    "            dict_df = dict_df[dict_df[id_column] != term]\n",
    "        # drop all null values\n",
    "        dict_df.dropna(inplace=True)\n",
    "        # sort values by 'CONSIGNOR_NAME'\n",
    "        dict_df.sort_values(name_column, inplace = True) \n",
    "        # dropping ALL duplicte 'CONSIGNOR_NAME' values from dictionary\n",
    "        dict_df.drop_duplicates(subset =name_column, keep = 'first', inplace = True) \n",
    "        # create list of 2-item lists: [CONSIGNOR_NAME, CONSIGNOR_INN]\n",
    "        new_list = dict_df.values.tolist()\n",
    "        # create dictionary out of list of lists\n",
    "        # for every list in the list of lists, take the first item in list (CONSIGNOR_NAME)\n",
    "        # and add it to index position of dictionary, take second term ('CONSIGNOR_INN') and add it to value position of dictionary\n",
    "        # cannot use pandas.to_dict() because it adds column names to dictionary; only want indexes/values\n",
    "        new_dict = {t[0]:t[1] for t in new_list}\n",
    "        # map new_dict to 'CONSIGNOR_INN' column of main dataframe\n",
    "        df[id_column] = df[name_column].map(new_dict)\n",
    "\n",
    "        # drop null values\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # remove all rows from list whose total INN count is less than min_trades variable\n",
    "        # way to limit size before processing, weed out INNs that only have a few trades present in dataset\n",
    "        df = df[df.groupby(id_column)[id_column].transform('size') >= min_trades]\n",
    "        \n",
    "        #create list for preprocessed text to be appended to\n",
    "        processed_text_list = []\n",
    "        \n",
    "        #this is the alg to apply preprocessing function to text column\n",
    "        # removed print statement from David's function\n",
    "        for i in range(len(df[text_column])):\n",
    "            x = df[text_column].iloc[i]\n",
    "            if isinstance(x, str):\n",
    "                processed_text_list.append(preprocess_text(x))\n",
    "            else:\n",
    "                processed_text_list.append(preprocess_text(x.astype(str)))\n",
    "            \n",
    "        # convert list of preprocessed text to dataframe\n",
    "        # to be concatenated onto original dataframe\n",
    "        df1 = pd.DataFrame({'PREPROCESSED_TEXT':processed_text_list})\n",
    "        \n",
    "        # reset indices of both dataframes\n",
    "        df1 = df1.reset_index()\n",
    "        df = df.reset_index()\n",
    "        df['index'] = df.index\n",
    "        \n",
    "        # merge preprocessed text to original dataframe\n",
    "        df_merge = pd.concat([df, df1], axis=1, join='inner')\n",
    "        \n",
    "        # drop DESCRIPTION_GOOD column, no longer necessary now that PROCESSED_TEXT column is present\n",
    "        df_merge = df_merge.drop([text_column, 'index'], axis='columns')\n",
    "        \n",
    "        #define variable to feed to TFIDF Vectorizer - 'PROCESSED_TEXT' column of train dataset\n",
    "        text = df_merge['PREPROCESSED_TEXT']\n",
    "        \n",
    "        #transform text with vectorizer\n",
    "        #Converted to Unicode because it will run into an np.nan error. This need to be turned into a unicode string.\n",
    "        sparse = vectorizer.transform(text.values.astype('U'))\n",
    "        \n",
    "        # Get feature names to use as dataframe column headers\n",
    "        dtm = pd.DataFrame(sparse.todense(), columns=vectorizer.get_feature_names())\n",
    "        \n",
    "        # reset indices of both dataframes for merge\n",
    "        # not sure why we had to do this, but running the following three commands gave us the results we wanted\n",
    "        dtm = dtm.reset_index()\n",
    "        df_merge = df_merge.reset_index()\n",
    "        df_merge['index'] = df_merge.index\n",
    "        dtm['index'] = dtm.index\n",
    "        \n",
    "        # merge vectorized word feature matrix with training dataset\n",
    "        df_merge_vector = pd.concat([df_merge, dtm], axis=1, join='inner')\n",
    "        # drop index columns\n",
    "        df_merge_vector = df_merge_vector.drop(columns=['index'])\n",
    "        \n",
    "        # variable manipulation to feed into KMeans model\n",
    "        # pull create variable containing dataframe of vectorized words only, all rows, columns indexed 4 and onward\n",
    "        X = df_merge_vector.drop(columns=[name_column, id_column, 'PREPROCESSED_TEXT'])\n",
    "        \n",
    "        # convert X dataframe into array\n",
    "        # necessary to feed to KMeans model\n",
    "        X_array = X.values\n",
    "        \n",
    "        # fit model on vectorized word array\n",
    "        labels = model.predict(X_array)\n",
    "        \n",
    "        # create 'cluster' column to add to vectorized dataframe\n",
    "        #Glue back to originaal data\n",
    "        df_merge_vector['cluster'] = labels\n",
    "\n",
    "        # extract columns for final analysis\n",
    "        Y = df_merge_vector[[id_column,'cluster']]\n",
    "        \n",
    "        # add column to dataframe for each cluster in model, created with copied values from 'cluster' column\n",
    "        # create 1,0 boolean to check if number in cell is equal to number of cluster, assigns 1s and 0s accordingly\n",
    "        # drop cluster column, no longer necessary now that we have count\n",
    "        for i in range(model.n_clusters):\n",
    "            Y['clust{}'.format(i)] = Y['cluster']\n",
    "            Y['clust{}'.format(i)] = (Y['clust{}'.format(i)] == i) * 1\n",
    "        \n",
    "        # drop 'cluster' column, no longer necessary now that we have total trades per cluster per INN\n",
    "        Y = Y.drop(columns=['cluster'])\n",
    "        \n",
    "        #create column_names variable to filter out CONSIGNER_INN from .groupby() in next step\n",
    "        column_names = Y.drop(columns = [id_column]).columns.tolist()\n",
    "        \n",
    "        #create new dataframe totalling trades per cluster per INN\n",
    "        Y = pd.DataFrame(Y.groupby([Y[id_column]])[column_names].sum()).reset_index()\n",
    "        \n",
    "        # add final tally for known arms exporters\n",
    "        # reset index so known arms exporters are at bottom of dataframe, indexed properly\n",
    "        Y = Y.append(clusters.iloc[0,1:], sort=None).reset_index().drop(columns=['index'])\n",
    "        \n",
    "        # convert all columns except for 'CONSIGNOR_INN' to decimals/percentages of total\n",
    "        Y[column_names] = Y[column_names].div(Y[column_names].sum(axis=1), axis=0)\n",
    "        \n",
    "        # cluster columns\n",
    "        # remove clusters with low percentages for known arms exporters from dataset\n",
    "        cluster_columns = cluster_columns\n",
    "        cluster_columns.insert(0, id_column)\n",
    "        Y = Y[cluster_columns]\n",
    "        \n",
    "        # similarity matrix - create list of p-distance scores using pdistance & euclidian distance\n",
    "        # simply put, it measures how similar two sets if numbers are\n",
    "        # https://stackoverflow.com/questions/35758612/most-efficient-way-to-construct-similarity-matrix\n",
    "        # each row in dataframe will be compared against the bottom row of the dataframe, which contains the totals for knowns arms exporters\n",
    "        pscores=[]\n",
    "        for i in range(len(Y)):\n",
    "            x = pdist([Y.iloc[-1, 1:],Y.iloc[i, 1:]], similarity_func)[0]\n",
    "            pscores.append(x)\n",
    "        \n",
    "        # add pdist_score column to Y dataframe\n",
    "        Y['pdist_score'] = pscores\n",
    "        \n",
    "        # drop control row (known arms exporters totals)\n",
    "        Y = Y.drop(Y.index[-1])\n",
    "        \n",
    "        # create profile_similarity_threshold variable\n",
    "        # if INN's pdist_score >= profile_similarity_threshold, INN will be included in final dataframe\n",
    "        # if INN's pdist_score < profile_similarity_threshold, INN will not be included in final dataframe\n",
    "        Y = Y[Y['pdist_score'] >= profile_similarity_threshold]\n",
    "        \n",
    "        #generate dataframe\n",
    "        return Y\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHIPPER_EDRPOU</th>\n",
       "      <th>clust0</th>\n",
       "      <th>clust1</th>\n",
       "      <th>pdist_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>692096</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>0.973094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>35947117</td>\n",
       "      <td>0.806387</td>\n",
       "      <td>0.127745</td>\n",
       "      <td>0.927716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4146</th>\n",
       "      <td>41633830</td>\n",
       "      <td>0.804245</td>\n",
       "      <td>0.101415</td>\n",
       "      <td>0.924665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>34589850</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.917974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>30638249</td>\n",
       "      <td>0.752747</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.916215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>41330519</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.913599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>39695169</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.909903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>382102</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.909726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>37987502</td>\n",
       "      <td>0.791980</td>\n",
       "      <td>0.208020</td>\n",
       "      <td>0.909659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>33240672</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.909477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>23510703</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.908660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>413475</td>\n",
       "      <td>0.818671</td>\n",
       "      <td>0.181329</td>\n",
       "      <td>0.906682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>37310549</td>\n",
       "      <td>0.820043</td>\n",
       "      <td>0.179957</td>\n",
       "      <td>0.906226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>20925875</td>\n",
       "      <td>0.824859</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.906221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>377511</td>\n",
       "      <td>0.822496</td>\n",
       "      <td>0.177504</td>\n",
       "      <td>0.905345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>31804036</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.226131</td>\n",
       "      <td>0.905112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>24363204</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.229050</td>\n",
       "      <td>0.903931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>39792657</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.903547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>382272</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.902493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>38884736</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.902002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>36417791</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.900618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>36767366</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.900509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>31489175</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.899222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>32502825</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>0.898292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>381143</td>\n",
       "      <td>0.837963</td>\n",
       "      <td>0.162037</td>\n",
       "      <td>0.898025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SHIPPER_EDRPOU    clust0    clust1  pdist_score\n",
       "4439         692096  0.756098  0.134146     0.973094\n",
       "2475       35947117  0.806387  0.127745     0.927716\n",
       "4146       41633830  0.804245  0.101415     0.924665\n",
       "2214       34589850  0.816667  0.116667     0.917974\n",
       "1144       30638249  0.752747  0.219780     0.916215\n",
       "4023       41330519  0.755102  0.040816     0.913599\n",
       "3484       39695169  0.794872  0.205128     0.909903\n",
       "3086         382102  0.804348  0.195652     0.909726\n",
       "3012       37987502  0.791980  0.208020     0.909659\n",
       "1918       33240672  0.790476  0.209524     0.909477\n",
       "589        23510703  0.785714  0.214286     0.908660\n",
       "4031         413475  0.818671  0.181329     0.906682\n",
       "2809       37310549  0.820043  0.179957     0.906226\n",
       "364        20925875  0.824859  0.169492     0.906221\n",
       "2939         377511  0.822496  0.177504     0.905345\n",
       "1477       31804036  0.773869  0.226131     0.905112\n",
       "656        24363204  0.770950  0.229050     0.903931\n",
       "3532       39792657  0.826923  0.173077     0.903547\n",
       "3093         382272  0.829268  0.170732     0.902493\n",
       "3248       38884736  0.766667  0.233333     0.902002\n",
       "2573       36417791  0.837989  0.117318     0.900618\n",
       "2652       36767366  0.833333  0.166667     0.900509\n",
       "1394       31489175  0.761194  0.238806     0.899222\n",
       "1708       32502825  0.759494  0.240506     0.898292\n",
       "3057         381143  0.837963  0.162037     0.898025"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = process_predictor_function(X, name_column = 'SHIPPER_NAME', id_column = 'SHIPPER_EDRPOU', text_column = 'DESCRIPTION_GOODS',\n",
    "                               profile_similarity_threshold = .85, cluster_columns = ['clust0', 'clust1'])\n",
    "test.sort_values(by='pdist_score', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
