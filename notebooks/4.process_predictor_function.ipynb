{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 - Text Processing/Predicting Function\n",
    "The predictor_function, which analyzes a dataframe's text column for similarities with known arms exporters, is housed in this notebook.  Throughout, markdown kernels will explain the steps and functionality contained in each portion of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIP Installations\n",
    "The following pip installations are required to get the predictor_function to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip installations - necessary to get notebook to run\n",
    "#update dask\n",
    "!pip install --upgrade pip\n",
    "!pip install dask==2.4.8\n",
    "!pip install fsspec\n",
    "!pip install --upgrade s3fs\n",
    "!pip install numpy\n",
    "!pip install pymystem3\n",
    "!pip install spacy\n",
    "!pip install joblib\n",
    "!pip install pymorphy2==0.8\n",
    "!pip install dask_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Imports from the first three notebooks, as well as additional imports 'euclidian' & 'pdist', are housed here.  Euclidian and pdistance are statistical measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/dask/array/random.py:27: FutureWarning: dask.array.random.doc_wraps is deprecated and will be removed in a future version\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# dataframe\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# DESCRIPTION_GOOD preprocessing\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "# machine learning/analysis\n",
    "import dask_ml.cluster as dask_ml_model # sklearn's skmeans took up too much memory to run.\n",
    "\n",
    "# measuring euclidian distance\n",
    "from scipy.spatial.distance import euclidean, pdist\n",
    "\n",
    "# S3 bucket interaction\n",
    "import tempfile\n",
    "import boto3\n",
    "import joblib\n",
    "\n",
    "# Disable warning message related to SettingWithCopyWarning\n",
    "# displays when running final function otherwise\n",
    "pd.options.mode.chained_assignment = None     # default = 'warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions, Classes, and Stopwords\n",
    "Functions, classes, and stopwords from notebooks 2 and 3 are housed here.  The similarity_func is also housed here, which will be used as a part of the function's final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to /home/ec2-user/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# define stemmer and Russian stopwords for data preprocessing\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "# https://stackoverflow.com/questions/5511708/adding-words-to-nltk-stoplist\n",
    "# add trade-specific stopwords to list\n",
    "newStopWords = ['г', '№', '10', '1', '20', '30', 'кг', '5', 'см',\n",
    "                '100', '80', '2', 'х', 'l', 'м', '00', '000'\n",
    "                '1.27', '2011.10631', '4', '12', '3', 'фр', 'количество',\n",
    "                'становиться', 'мм', 'вид', 'упаковка', 'получать',\n",
    "                'прочий', 'использование', 'масса', 'размер', 'черный',\n",
    "                '6', '8', '7', '50', '40', '25', 'коробка', 'поддон',\n",
    "                'вдоль', '250', '65', '85', '15', '35', '40', '45',\n",
    "                '55', '60', '70', '75', 'м3', '13', '0', '14',\n",
    "                '16', '18', 'm2', 'п', 'р', 'т', 'тип', 'являться',\n",
    "                'размер', 'cm', 'm', '01', '02', '03', '04', '05',\n",
    "                '06', '07', '08', '09', '24', '27']\n",
    "russian_stopwords.extend(newStopWords)\n",
    "\n",
    "#define function for preprocessing text - to be used later in notebook\n",
    "#function will remove Russian stop words and any punctuation not removed in cleaning_trade_data_desc_kmeans.ipynb\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "        and token != \" \" \\\n",
    "        and token.strip() not in punctuation]\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "# similarity function for euclidian measure at end of main function\n",
    "def similarity_func(u, v):\n",
    "    return 1/(1+euclidean(u,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Imports & Known Russian Arms Exporter Definition\n",
    "For the function to work, the vectorizer, model, and listArmscluster (armsclustersf.csv) need to be loaded into the notebook from the S3 Bucket.  Additonally, the list of known arms exporters needs to be copied over to notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model, vectorizer, and tokenizer to notebook\n",
    "s3 = boto3.resource('s3')\n",
    "bucket=s3.Bucket('labs20-arms-bucket')\n",
    "\n",
    "# load vectorizer from S3 bucket\n",
    "key = \"vectorizerf.pkl\"\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    bucket.download_fileobj(Fileobj=fp, Key=key)\n",
    "    fp.seek(0)\n",
    "    vectorizer = joblib.load(fp)\n",
    "\n",
    "# load model from S3 bucket\n",
    "key = \"modelf.pkl\"\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    bucket.download_fileobj(Fileobj=fp, Key=key)\n",
    "    fp.seek(0)\n",
    "    model = joblib.load(fp)\n",
    "\n",
    "#load cluster dataset from S3 bucket\n",
    "# drop error column accidentally created in import\n",
    "clusters = pd.read_csv('s3://labs20-arms-bucket/data/armsclustersf.csv')\n",
    "clusters = clusters.drop([clusters.columns[0]], axis='columns')\n",
    "\n",
    "# list of known arms exporters\n",
    "inn_arms_exp_total = ['7718852163',  '7740000090',    '7731084175',  '6161021690',\n",
    "                      '3807002509',  '6672315362',    '7802375335',  '7813132895',  \n",
    "                      '7731280660',  '7303026762',    '5040007594',  '2501002394',  \n",
    "                      '7807343496',  '7731559044',    '5042126251',  '7731595540',    \n",
    "                      '7733018650',  '7722016820',    '7705654132',  '7714336520',    \n",
    "                      '7801074335',  '6229031754',    '7830002462',  '6825000757',  \n",
    "                      '5043000212',  '7802375889',    '5010031470',  '1660249187',  \n",
    "                      '7720015691',  '6154573235',    '5038087144',  '7713006304',  \n",
    "                      '7805326230',  '5023002050',    '4007017378',  '7714013456',  \n",
    "                      '17718852163', '7811406004',    '7702077840',  '7839395419',  \n",
    "                      '7702244226',  '7704721192',    '7731644035',  '7712040285',\n",
    "                      '7811144648',  '4345047310',    '7720066255',  '6607000556',\n",
    "                      '1832090230',  '1835011597',    '3305004083',  '4340000830',\n",
    "                      '5074051432',  '1841015504',    '7105008338',  '7106002829', \n",
    "                      '7704274402',  '5942400228',    '7105514574',  '5012039795', \n",
    "                      '7714733528',  '3904065550',    '6825000757',  '7807343496', \n",
    "                      '7731559044',  '7805231691',    '7704859803',  '0273008320',\n",
    "                      '7704274402',  '2902059091',    '7805034277',  '7727692011',\n",
    "                      '7733759899',  '6154028021',    '7328032711',  '2635002815',\n",
    "                      '5040097816',  '5027033274',    '5250018433',  '5200000046',\n",
    "                      '7743813961',  '7718016666',    '5047118550',  '7704274402']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.9, max_features=None,\n",
       "                min_df=0.01, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='full', copy_x=True, init='k-means||', init_max_iter=None,\n",
       "       max_iter=300, n_clusters=10, n_jobs=1, oversampling_factor=2,\n",
       "       precompute_distances='auto', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing clusters from final analysis/product\n",
    "Since clusters 2, 3, 4, 5, 7, 8, and 9 have little representation in known arms exporter dataset (account for less than 3% of total trades combined), we are removing them from the final similarity calculation.  The first iteration of our product assigned similarity scores that were too high that had low numbers of trades in these clusters.  This is incorrect, and removing them yielded much better final results.\n",
    "\n",
    "In other words, similarity will only be measured against clusters 0, 1, and 6.\n",
    "\n",
    "Removed clusters can be adjusted via the function's `cluster_columns` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONSIGNOR_INN</th>\n",
       "      <th>clust0</th>\n",
       "      <th>clust1</th>\n",
       "      <th>clust2</th>\n",
       "      <th>clust3</th>\n",
       "      <th>clust4</th>\n",
       "      <th>clust5</th>\n",
       "      <th>clust6</th>\n",
       "      <th>clust7</th>\n",
       "      <th>clust8</th>\n",
       "      <th>clust9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>known_AE_trade_count</td>\n",
       "      <td>55151</td>\n",
       "      <td>9961</td>\n",
       "      <td>81</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>8723</td>\n",
       "      <td>1356</td>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CONSIGNOR_INN  clust0  clust1  clust2  clust3  clust4  clust5  \\\n",
       "0  known_AE_trade_count   55151    9961      81      42      13      23   \n",
       "\n",
       "   clust6  clust7  clust8  clust9  \n",
       "0    8723    1356     348       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "#read df_trade_desc_processed_testIF2 for function test\n",
    "df = pd.read_csv('s3://labs20-arms-bucket/data/df_trade_desc_processed_testIF2.csv',dtype={'CONSIGNOR_INN': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2078002, 5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dataframe size\n",
    "# running test on dataframe containing 2,070,002 rows\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32185"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dataframe contains 32,185 unique INNS\n",
    "df['CONSIGNOR_INN'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONSIGNOR_NAME</th>\n",
       "      <th>DECLARATION_NUMBER</th>\n",
       "      <th>CONSIGNOR_INN</th>\n",
       "      <th>DESCRIPTION_GOOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ООО МАГНАТ</td>\n",
       "      <td>10607040/220817/0014872</td>\n",
       "      <td>3808198484</td>\n",
       "      <td>ЛЕСОМАТЕРИАЛЫРАСПИЛЕННЫЕ ВДОЛЬ Х/П ЛИСТВЕННИЦА...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ООО ТК ВЕСТА</td>\n",
       "      <td>10611020/250918/0025950</td>\n",
       "      <td>2465310231</td>\n",
       "      <td>ПИЛОМАТЕРИАЛЫ Х/П ЕЛЬ СИБИРСКАЯ PICEA OBOVATA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ООО ТЕХНОНИКОЛЬ - СТРОИТЕЛЬНЫЕ СИСТЕМЫ</td>\n",
       "      <td>10103110/060516/0009957</td>\n",
       "      <td>7702521529</td>\n",
       "      <td>ТЕПЛОИЗОЛЯЦИОННЫЕ ПЛИТЫ ПОРИСТЫЕ ИЗ ЭКСТРУЗИОН...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ЗАО ЭНЕРГОСТРОЙМОНТАЖ</td>\n",
       "      <td>10210350/170317/0004957</td>\n",
       "      <td>7813112708</td>\n",
       "      <td>КЛАПАНЫ ОБРАТНЫЕ ПОВОРОТНЫЕ ОДНОДИСКОВЫЕ ИЗГОТ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ООО КУПИШУЗ</td>\n",
       "      <td>10113110/040418/0043451</td>\n",
       "      <td>7705935687</td>\n",
       "      <td>РУБАШКА МУЖСКАЯ ШЕРСТЯНАЯ ТРИКОТАЖНАЯ НЕ КЛАСС...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           CONSIGNOR_NAME       DECLARATION_NUMBER  \\\n",
       "0                              ООО МАГНАТ  10607040/220817/0014872   \n",
       "1                            ООО ТК ВЕСТА  10611020/250918/0025950   \n",
       "2  ООО ТЕХНОНИКОЛЬ - СТРОИТЕЛЬНЫЕ СИСТЕМЫ  10103110/060516/0009957   \n",
       "3                   ЗАО ЭНЕРГОСТРОЙМОНТАЖ  10210350/170317/0004957   \n",
       "4                             ООО КУПИШУЗ  10113110/040418/0043451   \n",
       "\n",
       "  CONSIGNOR_INN                                   DESCRIPTION_GOOD  \n",
       "0    3808198484  ЛЕСОМАТЕРИАЛЫРАСПИЛЕННЫЕ ВДОЛЬ Х/П ЛИСТВЕННИЦА...  \n",
       "1    2465310231  ПИЛОМАТЕРИАЛЫ Х/П ЕЛЬ СИБИРСКАЯ PICEA OBOVATA ...  \n",
       "2    7702521529  ТЕПЛОИЗОЛЯЦИОННЫЕ ПЛИТЫ ПОРИСТЫЕ ИЗ ЭКСТРУЗИОН...  \n",
       "3    7813112708  КЛАПАНЫ ОБРАТНЫЕ ПОВОРОТНЫЕ ОДНОДИСКОВЫЕ ИЗГОТ...  \n",
       "4    7705935687  РУБАШКА МУЖСКАЯ ШЕРСТЯНАЯ ТРИКОТАЖНАЯ НЕ КЛАСС...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor Function\n",
    "The predictor function takes a Russian trade dataframe as an input and returns a dataframe of INNs whose trades' 'DESCRIPTION' columns are similar to those of known arms exporters.  The final threshold uses inverted euclidian distance and pdistance to measure how similar two sets of numbers are.  \n",
    "\n",
    "In short, the function compares the total trades per cluster ratios of known arms exporters against every INN in the input dataframe, and if the similarity between the ratios is above a certain threshold, that INN makes it into the final dataframe.  For example, if INN 'A' has 90% similar ratios to known arms exporters and the threshold is 65%, INN 'A' will be displayed in the function's final output.  If INN 'B' has 40% similar ratios to known arms exporters and the threshold is 65%, INN 'A' will not be displayed in the function's final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs:\n",
    "**df (required)** = input dataframe.  df must be a pandas dataframe* containing Russian trade data.  Must contain one company identifier and one trade description column\n",
    "\n",
    "**name_column (default = 'CONSIGNOR_NAME')** = name column to be used in function. Default is 'CONSIGNOR_NAME' because model was trained on dataset whose name column was 'CONSIGNOR_NAME'\n",
    "\n",
    "**id_column (default = 'CONSIGNOR_INN')** = name column to be used in function. Default is 'CONSIGNOR_INN' because model was trained on dataset whose id column was 'CONSIGNOR_INN'\n",
    "\n",
    "**text_column (default = 'DESCRIPTION_GOOD')** = name column to be used in function. Default is 'DESCRIPTION_GOOD' because model was trained on dataset whose text column was 'DESCRIPTION_GOOD'\n",
    "\n",
    "**invalid_id_terms (default = ['None', '00', 'ИНН/КПП НЕ О', '0'])** = invalid terms present in id column.  Will be used to clean dataset entries with invalid id numbers\n",
    "\n",
    "**min_trades (default = 35)** = minimum trades per INN to be considered by the function.  If a particular INN only has one or two trades in the input dataframe, the vectorized/predicted results might not be as accurate.  Typically the higher the min_trades number, the fewer entries there will be in the output dataframe\n",
    "\n",
    "**profile_similarity_threshold (default = .65)** = threshold for a given INN to appear in output dataframe.  If an INN's similarity score is below this threshold, it will not appear in the output dataframe.  If an INN's similarity score is above this threshold, it will appear in the output dataframe\n",
    "\n",
    "**cluster_columns = (default = ['clust0', 'clust1', 'clust6'])** = clusters of interest identified by the KMeans model.  Way of filtering out clusters that had little to no representation amongst known arms exporters.  Makes for a more accurate model.  Do not update unless `cluster` variable is also updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictor_function(df, name_column = 'CONSIGNOR_NAME', id_column = 'CONSIGNOR_INN', text_column = 'DESCRIPTION_GOOD',\n",
    "                              invalid_id_terms = ['None', '00', 'ИНН/КПП НЕ О', '0'], min_trades=35, profile_similarity_threshold = .65,\n",
    "                              cluster_columns = ['clust0', 'clust1', 'clust6']):\n",
    "    \"\"\"\n",
    "    function to clean INNs of input dataframe and return Russian arms exporter similarity calculation\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # set column variable\n",
    "        # reduce dataframe so that dataframe only contains columns in columns variable\n",
    "        df = df[[name_column, id_column, text_column]]\n",
    "        \n",
    "        # remove rows from dataset containing INNs of known arms exporters\n",
    "        # check 'INN' column against inn_arms_exp_total list, drop row if there's a match with the list\n",
    "        df = df[~df[id_column].isin(inn_arms_exp_total)]\n",
    "        \n",
    "        # clean INNs\n",
    "        # Create subslice of dataframe for dictionary\n",
    "        dict_df = df[[name_column, id_column]]\n",
    "        # clean columns of dict_df, remove invalid_id_terms from CONSIGNOR_INN column\n",
    "        invalid_id_terms = invalid_id_terms\n",
    "        for term in invalid_id_terms:\n",
    "            dict_df = dict_df[dict_df[id_column] != term]\n",
    "        # drop all null values\n",
    "        dict_df.dropna(inplace=True)\n",
    "        # sort values by 'CONSIGNOR_NAME'\n",
    "        dict_df.sort_values(name_column, inplace = True) \n",
    "        # dropping ALL duplicte 'CONSIGNOR_NAME' values from dictionary\n",
    "        dict_df.drop_duplicates(subset =name_column, keep = 'first', inplace = True) \n",
    "        # create list of 2-item lists: [CONSIGNOR_NAME, CONSIGNOR_INN]\n",
    "        new_list = dict_df.values.tolist()\n",
    "        # create dictionary out of list of lists\n",
    "        # for every list in the list of lists, take the first item in list (CONSIGNOR_NAME)\n",
    "        # and add it to index position of dictionary, take second term ('CONSIGNOR_INN') and add it to value position of dictionary\n",
    "        # cannot use pandas.to_dict() because it adds column names to dictionary; only want indexes/values\n",
    "        new_dict = {t[0]:t[1] for t in new_list}\n",
    "        # map new_dict to 'CONSIGNOR_INN' column of main dataframe\n",
    "        df[id_column] = df[name_column].map(new_dict)\n",
    "\n",
    "        # drop null values\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # remove all rows from list whose total INN count is less than min_trades variable\n",
    "        # way to limit size before processing, weed out INNs that only have a few trades present in dataset\n",
    "        df = df[df.groupby(id_column)[id_column].transform('size') >= min_trades]\n",
    "        \n",
    "        #create list for preprocessed text to be appended to\n",
    "        processed_text_list = []\n",
    "        \n",
    "        #this is the alg to apply preprocessing function to text column\n",
    "        # removed print statement from David's function\n",
    "        for i in range(len(df[text_column])):\n",
    "            x = df[text_column].iloc[i]\n",
    "            if isinstance(x, str):\n",
    "                processed_text_list.append(preprocess_text(x))\n",
    "            else:\n",
    "                processed_text_list.append(preprocess_text(x.astype(str)))\n",
    "            \n",
    "        # convert list of preprocessed text to dataframe\n",
    "        # to be concatenated onto original dataframe\n",
    "        df1 = pd.DataFrame({'PREPROCESSED_TEXT':processed_text_list})\n",
    "        \n",
    "        # reset indices of both dataframes\n",
    "        df1 = df1.reset_index()\n",
    "        df = df.reset_index()\n",
    "        df['index'] = df.index\n",
    "        \n",
    "        # merge preprocessed text to original dataframe\n",
    "        df_merge = pd.concat([df, df1], axis=1, join='inner')\n",
    "        \n",
    "        # drop DESCRIPTION_GOOD column, no longer necessary now that PROCESSED_TEXT column is present\n",
    "        df_merge = df_merge.drop([text_column, 'index'], axis='columns')\n",
    "        \n",
    "        #define variable to feed to TFIDF Vectorizer - 'PROCESSED_TEXT' column of train dataset\n",
    "        text = df_merge['PREPROCESSED_TEXT']\n",
    "        \n",
    "        #transform text with vectorizer\n",
    "        #Converted to Unicode because it will run into an np.nan error. This need to be turned into a unicode string.\n",
    "        sparse = vectorizer.transform(text.values.astype('U'))\n",
    "        \n",
    "        # Get feature names to use as dataframe column headers\n",
    "        dtm = pd.DataFrame(sparse.todense(), columns=vectorizer.get_feature_names())\n",
    "        \n",
    "        # reset indices of both dataframes for merge\n",
    "        # not sure why we had to do this, but running the following three commands gave us the results we wanted\n",
    "        dtm = dtm.reset_index()\n",
    "        df_merge = df_merge.reset_index()\n",
    "        df_merge['index'] = df_merge.index\n",
    "        dtm['index'] = dtm.index\n",
    "        \n",
    "        # merge vectorized word feature matrix with training dataset\n",
    "        df_merge_vector = pd.concat([df_merge, dtm], axis=1, join='inner')\n",
    "        # drop index columns\n",
    "        df_merge_vector = df_merge_vector.drop(columns=['index'])\n",
    "        \n",
    "        # variable manipulation to feed into KMeans model\n",
    "        # pull create variable containing dataframe of vectorized words only, all rows, columns indexed 4 and onward\n",
    "        X = df_merge_vector.drop(columns=[name_column, id_column, 'PREPROCESSED_TEXT'])\n",
    "        \n",
    "        # convert X dataframe into array\n",
    "        # necessary to feed to KMeans model\n",
    "        X_array = X.values\n",
    "        \n",
    "        # fit model on vectorized word array\n",
    "        labels = model.predict(X_array)\n",
    "        \n",
    "        # create 'cluster' column to add to vectorized dataframe\n",
    "        #Glue back to originaal data\n",
    "        df_merge_vector['cluster'] = labels\n",
    "\n",
    "        # extract columns for final analysis\n",
    "        Y = df_merge_vector[[id_column,'cluster']]\n",
    "        \n",
    "        # add column to dataframe for each cluster in model, created with copied values from 'cluster' column\n",
    "        # create 1,0 boolean to check if number in cell is equal to number of cluster, assigns 1s and 0s accordingly\n",
    "        # drop cluster column, no longer necessary now that we have count\n",
    "        for i in range(model.n_clusters):\n",
    "            Y['clust{}'.format(i)] = Y['cluster']\n",
    "            Y['clust{}'.format(i)] = (Y['clust{}'.format(i)] == i) * 1\n",
    "        \n",
    "        # drop 'cluster' column, no longer necessary now that we have total trades per cluster per INN\n",
    "        Y = Y.drop(columns=['cluster'])\n",
    "        \n",
    "        #create column_names variable to filter out CONSIGNER_INN from .groupby() in next step\n",
    "        column_names = Y.drop(columns = [id_column]).columns.tolist()\n",
    "        \n",
    "        #create new dataframe totalling trades per cluster per INN\n",
    "        Y = pd.DataFrame(Y.groupby([Y[id_column]])[column_names].sum()).reset_index()\n",
    "        \n",
    "        # add final tally for known arms exporters\n",
    "        # reset index so known arms exporters are at bottom of dataframe, indexed properly\n",
    "        Y = Y.append(clusters.iloc[0,1:], sort=None).reset_index().drop(columns=['index'])\n",
    "        \n",
    "        # convert all columns except for 'CONSIGNOR_INN' to decimals/percentages of total\n",
    "        Y[column_names] = Y[column_names].div(Y[column_names].sum(axis=1), axis=0)\n",
    "        \n",
    "        # cluster columns\n",
    "        # remove clusters with low percentages for known arms exporters from dataset\n",
    "        cluster_columns = cluster_columns\n",
    "        cluster_columns.insert(0, id_column)\n",
    "        Y = Y[cluster_columns]\n",
    "        \n",
    "        # similarity matrix - create list of p-distance scores using pdistance & euclidian distance\n",
    "        # simply put, it measures how similar two sets if numbers are\n",
    "        # https://stackoverflow.com/questions/35758612/most-efficient-way-to-construct-similarity-matrix\n",
    "        # each row in dataframe will be compared against the bottom row of the dataframe, which contains the totals for knowns arms exporters\n",
    "        pscores=[]\n",
    "        for i in range(len(Y)):\n",
    "            x = pdist([Y.iloc[-1, 1:],Y.iloc[i, 1:]], similarity_func)[0]\n",
    "            pscores.append(x)\n",
    "        \n",
    "        # add pdist_score column to Y dataframe\n",
    "        Y['pdist_score'] = pscores\n",
    "        \n",
    "        # drop control row (known arms exporters totals)\n",
    "        Y = Y.drop(Y.index[-1])\n",
    "        \n",
    "        # create profile_similarity_threshold variable\n",
    "        # if INN's pdist_score >= profile_similarity_threshold, INN will be included in final dataframe\n",
    "        # if INN's pdist_score < profile_similarity_threshold, INN will not be included in final dataframe\n",
    "        Y = Y[Y['pdist_score'] >= profile_similarity_threshold]\n",
    "        \n",
    "        #generate dataframe\n",
    "        return Y\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "Tested function on dataframe containing 2 million rows of trade data for 32,185 unique INNs.  Our function successfully analyzed these INNs, and based on the default profile similarity threshold of 65% reduced the dataset to 3,777 INNs of interest.  The text patterns of their trades' descriptions fall into a similar clustering pattern as know Russian arms exporters accorting to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONSIGNOR_INN</th>\n",
       "      <th>clust0</th>\n",
       "      <th>clust1</th>\n",
       "      <th>clust6</th>\n",
       "      <th>pdist_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>3123138830</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.935134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>610210391297</td>\n",
       "      <td>0.726829</td>\n",
       "      <td>0.063415</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.935869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>6119005430</td>\n",
       "      <td>0.723735</td>\n",
       "      <td>0.073930</td>\n",
       "      <td>0.147860</td>\n",
       "      <td>0.937712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3285</th>\n",
       "      <td>6166093748</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.938020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>6167129059</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.115226</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.939363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>7814413641</td>\n",
       "      <td>0.755682</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.142045</td>\n",
       "      <td>0.939392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>7839447850</td>\n",
       "      <td>0.761468</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.940256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>6145001168</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.945649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>5075018950</td>\n",
       "      <td>0.694074</td>\n",
       "      <td>0.175451</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>0.946850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>6164021988</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.137652</td>\n",
       "      <td>0.072874</td>\n",
       "      <td>0.946891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>6154135457</td>\n",
       "      <td>0.681303</td>\n",
       "      <td>0.110482</td>\n",
       "      <td>0.094901</td>\n",
       "      <td>0.947317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>7536146773</td>\n",
       "      <td>0.684685</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.103604</td>\n",
       "      <td>0.948108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>6154141429</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.948798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>7302040482</td>\n",
       "      <td>0.770115</td>\n",
       "      <td>0.098659</td>\n",
       "      <td>0.112069</td>\n",
       "      <td>0.949568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>7736207543</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.953639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>2311196470</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.124138</td>\n",
       "      <td>0.954208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>332701210896</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.955529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>6123015760</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.957165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>3442093695</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.957165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>6168094673</td>\n",
       "      <td>0.722772</td>\n",
       "      <td>0.118812</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.962778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2420</th>\n",
       "      <td>5042145938</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.963331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>6659007785</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.965847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>6161053692</td>\n",
       "      <td>0.715074</td>\n",
       "      <td>0.148897</td>\n",
       "      <td>0.091912</td>\n",
       "      <td>0.968969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>7814121367</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.976295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>7706216371</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.102857</td>\n",
       "      <td>0.978465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CONSIGNOR_INN    clust0    clust1    clust6  pdist_score\n",
       "1275    3123138830  0.775862  0.086207  0.137931     0.935134\n",
       "2941  610210391297  0.726829  0.063415  0.121951     0.935869\n",
       "2951    6119005430  0.723735  0.073930  0.147860     0.937712\n",
       "3285    6166093748  0.773333  0.133333  0.066667     0.938020\n",
       "3332    6167129059  0.666667  0.115226  0.123457     0.939363\n",
       "5093    7814413641  0.755682  0.079545  0.142045     0.939392\n",
       "5180    7839447850  0.761468  0.082569  0.091743     0.940256\n",
       "2996    6145001168  0.781250  0.109375  0.109375     0.945649\n",
       "2496    5075018950  0.694074  0.175451  0.121340     0.946850\n",
       "3190    6164021988  0.692308  0.137652  0.072874     0.946891\n",
       "3061    6154135457  0.681303  0.110482  0.094901     0.947317\n",
       "3955    7536146773  0.684685  0.162162  0.103604     0.948108\n",
       "3085    6154141429  0.686275  0.156863  0.137255     0.948798\n",
       "3843    7302040482  0.770115  0.098659  0.112069     0.949568\n",
       "4832    7736207543  0.685185  0.148148  0.129630     0.953639\n",
       "627     2311196470  0.741379  0.086207  0.124138     0.954208\n",
       "1376  332701210896  0.706897  0.172414  0.120690     0.955529\n",
       "2959    6123015760  0.725490  0.156863  0.078431     0.957165\n",
       "1411    3442093695  0.725490  0.156863  0.078431     0.957165\n",
       "3366    6168094673  0.722772  0.118812  0.079208     0.962778\n",
       "2420    5042145938  0.714286  0.166667  0.119048     0.963331\n",
       "3622    6659007785  0.700000  0.150000  0.125000     0.965847\n",
       "3123    6161053692  0.715074  0.148897  0.091912     0.968969\n",
       "5088    7814121367  0.707317  0.121951  0.121951     0.976295\n",
       "4208    7706216371  0.742857  0.142857  0.102857     0.978465"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "# test = process_predictor_function(df)\n",
    "test.sort_values(by='pdist_score', ascending=True).tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3777, 5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CONSIGNOR_INN    3777\n",
       "clust0           2256\n",
       "clust1           1170\n",
       "clust6           1406\n",
       "pdist_score      2963\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions & Recommendations for Future Groups\n",
    "**Conclusions:**\n",
    "- too much weight is given to cluster0.  Almost half the trade text in the training dataset were grouped into cluster0.  This skews the similarity score as well, as the majority of known arms exporter trades fall into cluster0\n",
    "\n",
    "**Recommendations:**\n",
    "- expand list of known arms exporters\n",
    "- add list of known non-arms exporters, will reduce CPU requirements of running function and improve results\n",
    "- explore hyperparameter tuning of KMeans model\n",
    "- explore other potential clustering/machine learning models\n",
    "- incorporate a larger vectorizer into the model. 301 columns does not fully capture the descriptions of 8 million trades\n",
    "- incorporate batch processing into model/vectorizer training.  Will allow for more accurate model, and alleviate memory issues Labs20 group encountered\n",
    "- incorporate more data!  Our model only focused on `NAME`, `ID`, and `TEXT` columns.  Analyzing more columns from the original dataset should yield a more accurate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
