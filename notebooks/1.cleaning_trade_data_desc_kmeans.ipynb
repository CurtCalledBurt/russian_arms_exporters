{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 - Initial PySpark Data Cleaning:\n",
    "The first step in the process was narrowing down the overall data to a subset we could work with.  Following Labs18 lead, we used a PySpark Sagemaker/EMR Cluster combination to do so.  In simple terms PySpark is Python module used to analyze large databases, and EMR Clusters are the linked CPUs that do the processing.  The initial dataset, `part-deb95738-54f1-4c84-84a0-3449af42f7c3.*.csv`, is too large to be loaded into a pandas or dask dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this notebook was to limit the dataset to columns we needed, and clean those columns for further analysis.  Although the process of setting up a PySpark SageMaker kernel backed by an EMR cluster is complicated for beginners, a step-by-step guide to the process can be found here: https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests connection to EMR cluster, will throw an error if everything isn't set up correctly\n",
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797fd60a1d5e4c0a88aa74caab580e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports packages and starts spark instance\n",
    "# (spark instance is started when the first cell of the notebook is run)\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Filters\n",
    "\n",
    "Our analysis was focused on the text in the trade descriptions, so we removed unrelated information from the original dataset.  Initially we considered the following five columns of data for our analysis:\n",
    "- `CONSIGNOR_NAME` - the name of the exporting company involved in the trade\n",
    "- `DECLARATION_NUMBER` - the 'trade id' number of the trade\n",
    "- `DIRECTION_TRANSLATED` - notation of whether the trade was an import or export\n",
    "- `CONSIGNOR_INN` - the id number of the exporting company involved in the trade\n",
    "- `DESCRIPTION_GOOD` - the description text of the trade, containing information about goods and quantities traded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdcf3fb5bcd4ebb974d45d75b16983a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# defines the columns we want to keep from the trade data\n",
    "# for initial KMeans analysis of description text, we will be keeping few columns\n",
    "# Labs18 Group had larger trade_columns list.  Because we focused our project on text vectorization and machine learning with NLP, we were concerned with \n",
    "# the DESCRIPTION_GOOD column and CONSIGNOR_INN\n",
    "trade_columns = ['CONSIGNOR_NAME'\n",
    "                ,'DECLARATION_NUMBER'\n",
    "                ,'DIRECTION_TRANSLATED'\n",
    "                ,'CONSIGNOR_INN'\n",
    "                ,'DESCRIPTION_GOOD'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff95d10cd88544f4bee4968b3cdd4f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reads russian trade data from s3\n",
    "# selects only columns from trade_columns\n",
    "df_trade = spark.read.options(header=True, inferSchema=True, delimiter='|')\\\n",
    "              .csv('s3://russia-trade-data/2019_09_25_21_27_50/part-deb95738-54f1-4c84-84a0-3449af42f7c3.*.csv')\\\n",
    "              .select(trade_columns)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegEx\n",
    "\n",
    "Following Labs18's lead, we used RegEx to filter out import trades (leaving only exports) and clean the information in the `CONSIGNOR_INN` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880939ff39704cdd9d900941fe5958aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filters to keep only the rows for trade exports, as C4 ARMS project is focused on exporters (for now)\n",
    "df_trade = df_trade.filter(df_trade['DIRECTION_TRANSLATED'] == 'EXPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820ed52482e545e4b249b350754c7d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# defines regex expressions to apply to trade data\n",
    "# for initial KMeans analysis of description text, we will only be using regex filter for INN, as correct INNs are critical for our final analysis\n",
    "# regexINN filter needs improvement.  Invalid INN numbers were found in the output of the final product.  Cleaning these incorrect INNs would make the \n",
    "# final product/model.\n",
    "regexINN = '(\\d{8,12}|None|null|0|00)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ca0d6b50aa4ea8886bdaca54730636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# applies Lab18's Jason's regex to the trade data\n",
    "# removes rows that don't fit the regex filter from the trade dataset\n",
    "df_trade_filtered = df_trade.filter(df_trade['CONSIGNOR_INN'].rlike(regexINN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounting for Errors\n",
    "Before continuing with text vectorization, we had to remove some punctuation characters from the `DESCRIPTION_GOOD` column, as the punctuation was causing .csv import errors in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3b785026624afbb97640d56fd1d6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here, Labs20 group removed commas, hyphens, semi-colons, colons, apostrophes, and quotation marks from the dataset's DESCRIPTION_GOOD column\n",
    "# Without this step, the Python3 Kernel notebooks that analyze the data have trouble loading in the .csv file, as read_csv interprets the presence\n",
    "# of some of these punctuation marks as column separators.\n",
    "df = df_trade_filtered.withColumn(\"DESCRIPTION_GOOD\",regexp_replace(\"DESCRIPTION_GOOD\", \",\", \"\"))\n",
    "df = df.withColumn(\"DESCRIPTION_GOOD\",regexp_replace(\"DESCRIPTION_GOOD\", \"-\", \"\"))\n",
    "df = df.withColumn(\"DESCRIPTION_GOOD\",regexp_replace(\"DESCRIPTION_GOOD\", \";\", \"\"))\n",
    "df = df.withColumn(\"DESCRIPTION_GOOD\",regexp_replace(\"DESCRIPTION_GOOD\", \":\", \"\"))\n",
    "df = df.withColumn(\"DESCRIPTION_GOOD\",regexp_replace(\"DESCRIPTION_GOOD\", \"'\", \"\"))\n",
    "df = df.withColumn(\"DESCRIPTION_GOOD\",regexp_replace(\"DESCRIPTION_GOOD\", '\"', \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc1a3a942ee4d6ea4ba4a1b7da50650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# drop unwanted columns.  The DIRECTION_TRANSLATED column is no longer valuable now that the dataframe contains exports only\n",
    "df = df.drop('DIRECTION_TRANSLATED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284f5263080749ac8c602f9f31a3bc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# saves dataframe to S3\n",
    "df.write.save('s3://labs20-arms-bucket/data/df_trade_data_desc.csv', format='csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
